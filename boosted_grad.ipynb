{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "# Temp until our functions are complete\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(*arrays, test_size = 0.2, training_size = 0.8, random_state = None):\n",
    "    if test_size+training_size!=1:\n",
    "        raise 'Bad training/test split size'\n",
    "    for i in arrays:\n",
    "        if len(i)!=len(arrays[0]):\n",
    "            raise 'Bad input object size'\n",
    "    random.seed(random_state)\n",
    "    array_size = len(arrays[0])\n",
    "    num_test = math.floor(array_size*test_size)\n",
    "    num_train = math.floor(array_size*training_size)\n",
    "    num_train+=(array_size-num_train-num_test)\n",
    "    return_list = []\n",
    "    test_list = random.sample(range(array_size),num_test, )\n",
    "    for i in arrays:\n",
    "        if type(i)==pd.DataFrame:\n",
    "            test_array = pd.DataFrame(columns = X.columns)\n",
    "            train_array = pd.DataFrame(columns = X.columns)\n",
    "        elif type(i)==pd.Series:\n",
    "            test_array = pd.Series()\n",
    "            train_array = pd.Series()\n",
    "        test_array = i.iloc[test_list]\n",
    "        train_array = i.drop(test_list)\n",
    "        return_list.append(train_array)\n",
    "        return_list.append(test_array)\n",
    "    return return_list\n",
    "    \n",
    "def my_mse(a, b):\n",
    "    a = a.to_numpy()\n",
    "    sum = 0\n",
    "    for i in range(len(a)):\n",
    "        sum+=(a[i]-b[i])**2\n",
    "    sum/=len(a)\n",
    "    return sum\n",
    "\n",
    "def my_rmse(a, b):\n",
    "    return math.sqrt(my_mse(a,b))\n",
    "\n",
    "def my_mae(a,b):\n",
    "    a = a.to_numpy()\n",
    "    sum = 0\n",
    "    for i in range(len(a)):\n",
    "        sum+=abs(a[i]-b[i])\n",
    "    sum/=len(a)\n",
    "    return sum\n",
    "\n",
    "def my_r2(a,b):\n",
    "    a = a.to_numpy()\n",
    "    y_bar = sum(a)/len(a)\n",
    "    ss_res = 0\n",
    "    ss_tot = 0\n",
    "    for i in range(len(a)):\n",
    "        ss_res+=(a[i]-b[i])**2\n",
    "    for j in range(len(a)):\n",
    "        ss_tot+=(a[j]-y_bar)**2\n",
    "    return 1-(ss_res/ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used as a week learned within the gradient boosting ensemble (builds iteratively which is why there is a max depth of 1)\n",
    "# High bias but low variance, \n",
    "# Ideal for boosting methods that aim to iteratively reduce error by focusing on hard-to-predict instances.\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    # Each node is a decision point for traversing through the decision tree\n",
    "    # Follows decision tree logic\n",
    "    class Node:\n",
    "        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "            # feature_Index and threshold are determining factors for where the split happens\n",
    "            self.feature_index = feature_index\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    # Recursive binary splitting\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Continues to split until the maximum specified depth is reached\n",
    "        num_samples, num_features = X.shape\n",
    "        # Stopping condition: if the current depth exceeds the max depth or the dataset cannot be split further\n",
    "        if depth >= self.max_depth or num_samples <= 1:\n",
    "            leaf_value = self._calculate_leaf_value(y)\n",
    "            # Create a leaf node with the calculated value\n",
    "            return self.Node(value=leaf_value)\n",
    "\n",
    "        # Finds the best features by iterating through and selecting lowest MSE\n",
    "        best_feature, best_threshold = self._find_best_split(X, y, num_samples, num_features)\n",
    "        if best_feature is None:\n",
    "            # If no split can improve the outcome, create a leaf node\n",
    "            return self.Node(value=self._calculate_leaf_value(y))\n",
    "        \n",
    "        # Split the dataset and recursively build left and right subtrees\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
    "        left_subtree = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return self.Node(feature_index=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def _calculate_leaf_value(self, y):\n",
    "        # The leaf value can be the mean of the target values, calculates that mean value for the leaf node\n",
    "        return np.mean(y)\n",
    "\n",
    "    def _find_best_split(self, X, y, num_samples, num_features):\n",
    "        # Finds the best feature and threshold to split on based on the lowest MSE\n",
    "        best_feature, best_threshold = None, None\n",
    "        best_mse = np.inf\n",
    "        for feature_index in range(num_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            # Splits the dataset and calculates the MSE for this split\n",
    "            for threshold in thresholds:\n",
    "                left_idxs, right_idxs = self._split(X[:, feature_index], threshold)\n",
    "                if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                    continue\n",
    "                mse = self._calculate_mse(y[left_idxs], y[right_idxs])\n",
    "                # Update split if current mse is better\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # Splits dataset into left/right based on threshold of given feature\n",
    "    def _split(self, feature_values, threshold):\n",
    "        left_idxs = np.where(feature_values <= threshold)[0]\n",
    "        right_idxs = np.where(feature_values > threshold)[0]\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _calculate_mse(self, left_y, right_y):\n",
    "        # Calculate the MSE of the left and right splits by weighted averages of variance\n",
    "        total_left_mse = np.var(left_y) * len(left_y) if len(left_y) > 0 else 0\n",
    "        total_right_mse = np.var(right_y) * len(right_y) if len(right_y) > 0 else 0\n",
    "        total_mse = (total_left_mse + total_right_mse) / (len(left_y) + len(right_y))\n",
    "        return total_mse\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predictions array to store predictions for each sample in X\n",
    "        predictions = np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "        return predictions\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        # Recursive method to traverse the tree for a single sample 'x' until a leaf node is reached\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoost:\n",
    "    def __init__(self, n_estimators: int = 25, max_depth: int = 1, learning_rate: int =.1):\n",
    "        self.max_depth = max_depth # Max depth of the trees\n",
    "        self.n_estimators = n_estimators # Number of trees\n",
    "        self.learning_rate = learning_rate # Learning rate, step size for parameter update\n",
    "        self.trees = [] # List of our trees\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # To start all residuals = y_train\n",
    "        residuals = np.copy(y_train)\n",
    "        self.f_hat = 0 \n",
    "        # Now time to make decision trees\n",
    "        for i in range(self.n_estimators):\n",
    "            # Build and Fit Tree to data\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X_train, residuals)\n",
    "            # Save our tree\n",
    "            self.trees.append(tree)\n",
    "            # Make prediction\n",
    "            f_hat_b = tree.predict(X_train)\n",
    "            # Update f_hat\n",
    "            self.f_hat += (self.learning_rate*f_hat_b) \n",
    "            # Update residuals\n",
    "            residuals = residuals - (self.learning_rate*f_hat_b)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_hat = np.zeros((X_test.shape[0], ))\n",
    "        for tree in self.trees:\n",
    "            y_hat += self.learning_rate*tree.predict(X_test)\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostAll:\n",
    "    def __init__(self, n_estimators: int = 25, max_depth: int = 1, learning_rate: int =.1):\n",
    "        self.max_depth = max_depth # Max depth of the trees\n",
    "        self.n_estimators = n_estimators # Number of trees\n",
    "        self.learning_rate = learning_rate # Learning rate, step size for parameter update\n",
    "        self.trees = [] # List of our trees\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # To start all residuals = y_train\n",
    "        residuals = np.copy(y_train)\n",
    "        self.f_hat = 0 \n",
    "        # Now time to make decision trees\n",
    "        for i in range(self.n_estimators):\n",
    "            # Build and Fit Tree to data\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X_train, residuals)\n",
    "            # Save our tree\n",
    "            self.trees.append(tree)\n",
    "            # Make prediction\n",
    "            f_hat_b = tree.predict(X_train)\n",
    "            # Update f_hat\n",
    "            self.f_hat += (self.learning_rate*f_hat_b) \n",
    "            # Update residuals\n",
    "            residuals = residuals - (self.learning_rate*f_hat_b)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_hat = np.zeros((X_test.shape[0], ))\n",
    "        for tree in self.trees:\n",
    "            y_hat += self.learning_rate*tree.predict(X_test)\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\EmmaBland\\AppData\\Local\\Temp\\ipykernel_19816\\2391172120.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df = pd.read_csv('housing.csv', sep = '\\s+', header = None, names = column_names)\n"
     ]
    }
   ],
   "source": [
    "# Data \n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv('housing.csv', sep = '\\s+', header = None, names = column_names) \n",
    "df.head()\n",
    "\n",
    "# Remove Outliers\n",
    "upper_QR = df['MEDV'].quantile(0.75)\n",
    "lower_QR = df['MEDV'].quantile(0.25)\n",
    "inter_QR = upper_QR-lower_QR\n",
    "low_outliers = df['MEDV']<(lower_QR-(1.5*inter_QR))\n",
    "high_outliers = df['MEDV']>(upper_QR+(1.5*inter_QR))\n",
    "#print(upper_QR, lower_QR, inter_QR)\n",
    "#print(low_outliers.value_counts(), high_outliers.value_counts())\n",
    "\n",
    "remove_outliers = df[df['MEDV']<(upper_QR+(1.5*inter_QR))]\n",
    "remove_outliers = remove_outliers[remove_outliers['MEDV']>(lower_QR-(1.5*inter_QR))]\n",
    "#print(remove_outliers)\n",
    "\n",
    "X = remove_outliers[['LSTAT', 'RM', 'TAX', 'PTRATIO', 'CRIM', 'NOX']]\n",
    "y = remove_outliers['MEDV']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=123)\n",
    "\n",
    "# Turn to np for now\n",
    "X_train_np = X_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_test_np = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\EmmaBland\\AppData\\Local\\Temp\\ipykernel_19816\\1851819894.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  df = pd.read_csv('housing.csv', sep = '\\s+', header = None, names = column_names)\n"
     ]
    }
   ],
   "source": [
    "# Data \n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv('housing.csv', sep = '\\s+', header = None, names = column_names) \n",
    "df.head()\n",
    "\n",
    "# Remove Outliers\n",
    "upper_QR = df['MEDV'].quantile(0.75)\n",
    "lower_QR = df['MEDV'].quantile(0.25)\n",
    "inter_QR = upper_QR-lower_QR\n",
    "low_outliers = df['MEDV']<(lower_QR-(1.5*inter_QR))\n",
    "high_outliers = df['MEDV']>(upper_QR+(1.5*inter_QR))\n",
    "#print(upper_QR, lower_QR, inter_QR)\n",
    "#print(low_outliers.value_counts(), high_outliers.value_counts())\n",
    "\n",
    "remove_outliers = df[df['MEDV']<(upper_QR+(1.5*inter_QR))]\n",
    "remove_outliers = remove_outliers[remove_outliers['MEDV']>(lower_QR-(1.5*inter_QR))]\n",
    "#print(remove_outliers)\n",
    "\n",
    "X = remove_outliers[['LSTAT', 'RM', 'TAX', 'PTRATIO', 'CRIM', 'NOX']]\n",
    "y = remove_outliers['MEDV']\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "#print(X,y, \"X: \", type(X), \"y: \", type(y))\n",
    "#X = df[['LSTAT', 'RM', 'TAX', 'PTRATIO', 'CRIM', 'NOX']]\n",
    "#y = df['MEDV']\n",
    "#print(X,y, \"X: \", type(X), \"y: \", type(y))\n",
    "X_train, X_test, y_train, y_test = my_train_test_split(X,y,test_size=0.2, training_size=.8, random_state=123)\n",
    "\n",
    "# Turn to np for now\n",
    "X_train_np = X_train.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "y_test_np = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4.925757849083055\n",
      "MAE: 1.7181155441640121\n",
      "R2: 0.873507230715757\n"
     ]
    }
   ],
   "source": [
    "# Testing Gradient Boost Regression\n",
    "test = GradientBoostAll(n_estimators= 40, max_depth=3)\n",
    "test.fit(X_train_np, y_train_np)\n",
    "predictions = test.predict(X_test.to_numpy())\n",
    "print(\"MSE:\", mean_squared_error(y_test, predictions))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, predictions))\n",
    "print(\"R2:\", r2_score(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 5.009495374883542\n",
      "MAE: 1.75631866620722\n",
      "R2: 0.8713568628219137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\EmmaBland\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Testing Gradient Boost 2 Regression\n",
    "test2 = GradientBoost(n_estimators= 40, max_depth=3)\n",
    "test2.fit(X_train_np, y_train_np)\n",
    "predictions2 = test2.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, predictions2))\n",
    "print(\"MAE:\", mean_absolute_error(y_test, predictions2))\n",
    "print(\"R2:\", r2_score(y_test, predictions2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
