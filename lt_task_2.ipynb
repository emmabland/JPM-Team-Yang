{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv('housing.csv', sep = '\\s+', header = None, names = column_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers(df, feature):\n",
    "    #first quartile\n",
    "    feature_q1 = df[feature].quantile(0.25)\n",
    "    #third quartile\n",
    "    feature_q3 = df[feature].quantile(0.75)\n",
    "    #interquartile range\n",
    "    feature_iqr = feature_q3 - feature_q1\n",
    "    #mask to eliminate low outliers\n",
    "    mask1 = df[feature]>feature_q1-1.5*feature_iqr\n",
    "    #mask to eliminate high outliers\n",
    "    mask3 = df[feature]<feature_q3+1.5*feature_iqr\n",
    "    return df[mask1][mask3].reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(*arrays, test_size = 0.2, training_size = 0.8, random_state = None):\n",
    "    if test_size+training_size!=1:\n",
    "        raise 'Bad training/test split size'\n",
    "    for i in arrays:\n",
    "        if len(i)!=len(arrays[0]):\n",
    "            raise 'Bad input object size'\n",
    "    random.seed(random_state)\n",
    "    array_size = len(arrays[0])\n",
    "    num_test = math.floor(array_size*test_size)\n",
    "    num_train = math.floor(array_size*training_size)\n",
    "    num_train+=(array_size-num_train-num_test)\n",
    "    return_list = []\n",
    "    test_list = random.sample(range(array_size),num_test, )\n",
    "    for i in arrays:\n",
    "        if type(i)==pd.DataFrame:\n",
    "            test_array = pd.DataFrame(columns = i.columns)\n",
    "            train_array = pd.DataFrame(columns = i.columns)\n",
    "        elif type(i)==pd.Series:\n",
    "            test_array = pd.Series()\n",
    "            train_array = pd.Series()\n",
    "        test_array = i.iloc[test_list]\n",
    "        train_array = i.drop(test_list)\n",
    "        return_list.append(train_array)\n",
    "        return_list.append(test_array)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(a, b):\n",
    "    if not isinstance(a, np.ndarray):\n",
    "        a = np.array(a)\n",
    "    if not isinstance(b, np.ndarray):\n",
    "        b = np.array(b)\n",
    "    return np.mean((a - b) ** 2)\n",
    "\n",
    "def my_rmse(a, b):\n",
    "    return math.sqrt(my_mse(a,b))\n",
    "\n",
    "def my_mae(a,b):\n",
    "    a = a.to_numpy()\n",
    "    sum = 0\n",
    "    for i in range(len(a)):\n",
    "        sum+=abs(a[i]-b[i])\n",
    "    sum/=len(a)\n",
    "    return sum\n",
    "\n",
    "def my_r2(a,b):\n",
    "    a = a.to_numpy()\n",
    "    y_bar = sum(a)/len(a)\n",
    "    ss_res = 0\n",
    "    ss_tot = 0\n",
    "    for i in range(len(a)):\n",
    "        ss_res+=(a[i]-b[i])**2\n",
    "    for j in range(len(a)):\n",
    "        ss_tot+=(a[j]-y_bar)**2\n",
    "    return 1-(ss_res/ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used as a week learned within the gradient boosting ensemble (builds iteratively which is why there is a max depth of 1)\n",
    "# High bias but low variance, \n",
    "# Ideal for boosting methods that aim to iteratively reduce error by focusing on hard-to-predict instances.\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    # Each node is a decision point for traversing through the decision tree\n",
    "    # Follows decision tree logic\n",
    "    class Node:\n",
    "        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "            # feature_Index and threshold are determining factors for where the split happens\n",
    "            self.feature_index = feature_index\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    # Recursive binary splitting\n",
    "    # Finds the best feature and threshold to split the data at each noted based on minimizing MSE\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # x - features, y - target, depth - depth of tree\n",
    "        # Continues to split until the maximum specified depth is reached\n",
    "        num_samples, num_features = X.shape\n",
    "        # Stopping condition: if the current depth exceeds the max depth set during initialization or the dataset cannot be split further\n",
    "        if depth >= self.max_depth or num_samples <= 1:\n",
    "            # Create a leaf node with the calculated value\n",
    "            leaf_value = self._calculate_leaf_value(y)\n",
    "            # Usually averages the target values in regression trees\n",
    "            return self.Node(value=leaf_value)\n",
    "\n",
    "        # Finds the best features by iterating through and selecting lowest MSE, calling find best split to find best feature/threshold\n",
    "        best_feature, best_threshold = self._find_best_split(X, y, num_samples, num_features)\n",
    "        # In scenarios where the MSE cannot be further decreased (no more effective split found)\n",
    "        if best_feature is None:\n",
    "            # If no split can improve the outcome, create a leaf node\n",
    "            return self.Node(value=self._calculate_leaf_value(y))\n",
    "        \n",
    "        # Split the dataset and recursively build left and right subtrees (<= left or > right)\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
    "        # Recursively calls build tree to construct subtrees on left and right increasing depth by 1 each time\n",
    "        left_subtree = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return self.Node(feature_index=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def _find_best_split(self, X, y, num_samples, num_features):\n",
    "        # Initializes variables to store best feature index\n",
    "        best_feature, best_threshold = None, None\n",
    "        # Used to insure any inital real MSE found is lower than this starting value\n",
    "        best_mse = np.inf\n",
    "        # Iterates through each feature in dataset X by index\n",
    "        for feature_index in range(num_features):\n",
    "            # extracts unique values of current features as potential thresholds for splitting, will change in future for midpoints or using minimizer\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            # SCIPI minimizer --- TODO\n",
    "            # Iterates through each unique threshold in current thresholds\n",
    "            for threshold in thresholds:\n",
    "                # Calls split function to divide dataset into two subsets based on threshold, left - where features are <= threshold, right - > threshold\n",
    "                left_idxs, right_idxs = self._split(X[:, feature_index], threshold)\n",
    "                # Check for if one subset becomes empty (all features fall onto right or left side)\n",
    "                if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                    continue\n",
    "                mse = self._calculate_mse(y[left_idxs], y[right_idxs])\n",
    "                # If MSE of current split is lower than best MSE found so far... \n",
    "                # Updates best MSE with new value and records the feature/ threshold of this achieved MSE\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    # Splits dataset into left/right based on threshold of given feature\n",
    "    def _split(self, feature_values, threshold):\n",
    "        left_idxs = np.where(feature_values <= threshold)[0]\n",
    "        right_idxs = np.where(feature_values > threshold)[0]\n",
    "        return left_idxs, right_idxs\n",
    "    \n",
    "    def _calculate_leaf_value(self, y):\n",
    "        # The leaf value can be the mean of the target values, calculates that mean value for the leaf node\n",
    "        return np.mean(y)\n",
    "\n",
    "    def _calculate_mse(self, left_y, right_y):\n",
    "        # Calculate the MSE of the left and right splits by weighted averages of variance\n",
    "        total_left_mse = np.var(left_y) * len(left_y) if len(left_y) > 0 else 0\n",
    "        total_right_mse = np.var(right_y) * len(right_y) if len(right_y) > 0 else 0\n",
    "        total_mse = (total_left_mse + total_right_mse) / (len(left_y) + len(right_y))\n",
    "        return total_mse\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predictions array to store predictions for each sample in X\n",
    "        predictions = np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "        return predictions\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        # Recursive method to traverse the tree for a single sample 'x' until a leaf node is reached\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_np = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train\n",
    "# y_train_np = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train\n",
    "# X_test_np = X_test.to_numpy() if isinstance(X_test, pd.DataFrame) else X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4386820326080767"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tree1 = DecisionTree()\n",
    "# Tree1.fit(X_train_np, y_train_np)\n",
    "# y_pred = Tree1.predict(X_test_np)\n",
    "# my_r2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _find_best_split(self, X, y, num_samples, num_features):\n",
    "#     best_feature, best_threshold = None, None\n",
    "#     best_mse = np.inf\n",
    "#     # Function to calculate MSE for a given threshold\n",
    "#     def mse_for_threshold(threshold, feature_index):\n",
    "#         left_idxs, right_idxs = self._split(X[:, feature_index], threshold)\n",
    "#         if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "#             return np.inf  # Return a large error if split is not feasible\n",
    "#         return self._calculate_mse(y[left_idxs], y[right_idxs])\n",
    "\n",
    "#     for feature_index in range(num_features):\n",
    "#         # Initial guess for the threshold can be the mean of the feature values\n",
    "#         initial_guess = np.mean(X[:, feature_index])\n",
    "#         # Bounds for the minimizer to ensure it stays within min and max of feature values\n",
    "#         bounds = [(np.min(X[:, feature_index]), np.max(X[:, feature_index]))]\n",
    "#         # The minimization process for the current feature\n",
    "#         result = minimize(mse_for_threshold, x0=initial_guess, args=(feature_index,), bounds=bounds, method='L-BFGS-B')\n",
    "#         if result.fun < best_mse:\n",
    "#             best_mse = result.fun\n",
    "#             best_feature = feature_index\n",
    "#             best_threshold = result.x[0]\n",
    "#     return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoost:\n",
    "    def __init__(self, n_estimators: int = 25, max_depth: int = 1, learning_rate: int =.5):\n",
    "        self.max_depth = max_depth # Max depth of the trees\n",
    "        self.n_estimators = n_estimators # Number of trees\n",
    "        self.learning_rate = learning_rate # Learning rate, step size for parameter update\n",
    "        self.trees = [] # List of our trees\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "            # Convert X_train and y_train to NumPy arrays if they aren't already\n",
    "        X_train = np.asarray(X_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        # To start all residuals = y_train\n",
    "        residuals = np.copy(y_train)\n",
    "        self.f_hat = 0 \n",
    "        # Now time to make decision trees\n",
    "        for i in range(self.n_estimators):\n",
    "            # Build and Fit Tree to data\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X_train, residuals)\n",
    "            # Save our tree\n",
    "            self.trees.append(tree)\n",
    "            # Make prediction\n",
    "            f_hat_b = tree.predict(X_train)\n",
    "            # Update f_hat\n",
    "            self.f_hat += (self.learning_rate*f_hat_b) \n",
    "            # Update residuals\n",
    "            residuals = residuals - (self.learning_rate*f_hat_b)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_hat = np.zeros((X_test.shape[0], ))\n",
    "        for tree in self.trees:\n",
    "            y_hat += self.learning_rate*tree.predict(X_test)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LukeTeitell\\AppData\\Local\\Temp\\ipykernel_16624\\2864869230.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return df[mask1][mask3].reset_index()\n"
     ]
    }
   ],
   "source": [
    "df = clean_outliers(df, 'MEDV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['RM',  'TAX', 'PTRATIO', 'LSTAT', 'INDUS']]\n",
    "y = df['MEDV']\n",
    "X_train, X_test, y_train, y_test = my_train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoost(max_depth=2, n_estimators=20)\n",
    "model.fit(X_train=X_train.to_numpy(), y_train=y_train.to_numpy())\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "print(my_mse(y_test, y_pred))\n",
    "print(my_mae(y_test, y_pred))\n",
    "print(my_r2(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in dictionary of parameters to explore\n",
    "def grid_search(X_train, y_train, X_test, y_test, param_grid):\n",
    "    # Track lowest MSE seeen so far, set to infinity initially so every value is lower\n",
    "    # Params stores best paramaters found through best score\n",
    "    best_score = float('inf')\n",
    "    best_params = {}\n",
    "\n",
    "    # Nested loops to iterate over every combination of estimators depth and LR\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                # Initalize and fit model with every combination of parameters\n",
    "                model = GradientBoost(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Make predictions on the test set and calculate MSE\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = my_mse(y_test, y_pred)\n",
    "\n",
    "                # Update best_score and best_params if current model is better\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate}\n",
    "\n",
    "    return best_params, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_np = np.asarray(X_train)\n",
    "y_train_np = np.asarray(y_train)\n",
    "X_test_np = np.asarray(X_test)\n",
    "y_test_np = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params, best_score = grid_search(X_train_np, y_train_np, X_test_np, y_test_np, param_grid)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best MSE Score:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
