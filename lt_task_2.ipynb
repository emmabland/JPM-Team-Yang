{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, ConfusionMatrixDisplay, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv('housing.csv', sep = '\\s+', header = None, names = column_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outliers(df, feature):\n",
    "    #first quartile\n",
    "    feature_q1 = df[feature].quantile(0.25)\n",
    "    #third quartile\n",
    "    feature_q3 = df[feature].quantile(0.75)\n",
    "    #interquartile range\n",
    "    feature_iqr = feature_q3 - feature_q1\n",
    "    #mask to eliminate low outliers\n",
    "    mask1 = df[feature]>feature_q1-1.5*feature_iqr\n",
    "    #mask to eliminate high outliers\n",
    "    mask3 = df[feature]<feature_q3+1.5*feature_iqr\n",
    "    return df[mask1][mask3].reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(*arrays, test_size = 0.2, training_size = 0.8, random_state = None):\n",
    "    if test_size+training_size!=1:\n",
    "        raise 'Bad training/test split size'\n",
    "    for i in arrays:\n",
    "        if len(i)!=len(arrays[0]):\n",
    "            raise 'Bad input object size'\n",
    "    random.seed(random_state)\n",
    "    array_size = len(arrays[0])\n",
    "    num_test = math.floor(array_size*test_size)\n",
    "    num_train = math.floor(array_size*training_size)\n",
    "    num_train+=(array_size-num_train-num_test)\n",
    "    return_list = []\n",
    "    test_list = random.sample(range(array_size),num_test, )\n",
    "    for i in arrays:\n",
    "        if type(i)==pd.DataFrame:\n",
    "            test_array = pd.DataFrame(columns = i.columns)\n",
    "            train_array = pd.DataFrame(columns = i.columns)\n",
    "        elif type(i)==pd.Series:\n",
    "            test_array = pd.Series()\n",
    "            train_array = pd.Series()\n",
    "        test_array = i.iloc[test_list]\n",
    "        train_array = i.drop(test_list)\n",
    "        return_list.append(train_array)\n",
    "        return_list.append(test_array)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(a, b):\n",
    "    if not isinstance(a, np.ndarray):\n",
    "        a = np.array(a)\n",
    "    if not isinstance(b, np.ndarray):\n",
    "        b = np.array(b)\n",
    "    return np.mean((a - b) ** 2)\n",
    "\n",
    "def my_rmse(a, b):\n",
    "    return math.sqrt(my_mse(a,b))\n",
    "\n",
    "def my_mae(a,b):\n",
    "    a = a.to_numpy()\n",
    "    sum = 0\n",
    "    for i in range(len(a)):\n",
    "        sum+=abs(a[i]-b[i])\n",
    "    sum/=len(a)\n",
    "    return sum\n",
    "\n",
    "def my_r2(a,b):\n",
    "    a = a.to_numpy()\n",
    "    y_bar = sum(a)/len(a)\n",
    "    ss_res = 0\n",
    "    ss_tot = 0\n",
    "    for i in range(len(a)):\n",
    "        ss_res+=(a[i]-b[i])**2\n",
    "    for j in range(len(a)):\n",
    "        ss_tot+=(a[j]-y_bar)**2\n",
    "    return 1-(ss_res/ss_tot)\n",
    "\n",
    "def my_performance(a: pd.DataFrame, b: pd.DataFrame):\n",
    "    \"\"\"Turns predicted and observed data to numpy ndarrays and carries out MSE, RMSE, MAE, and R2\n",
    "\n",
    "    Args:\n",
    "        a (pd.DataFrame): Observed Data\n",
    "        b (pd.DataFrame): Predicted Data\n",
    "    \"\"\"\n",
    "    if not isinstance(a, np.ndarray):\n",
    "        a=a.to_numpy()\n",
    "    if not isinstance(b, np.ndarray):\n",
    "        b=b.to_numpy()\n",
    "    print(f'Mean Squared Error: {my_mse(a,b)}')\n",
    "    print(f'Root Mean Squared Error: {my_rmse(a,b)}')\n",
    "    print(f'Mean Absolute Error: {my_mae(a,b)}')\n",
    "    print(f'R2 Score: {my_r2(a,b)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used as a week learned within the gradient boosting ensemble (builds iteratively which is why there is a max depth of 1)\n",
    "# High bias but low variance, \n",
    "# Ideal for boosting methods that aim to iteratively reduce error by focusing on hard-to-predict instances.\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    # Each node is a decision point for traversing through the decision tree\n",
    "    # Follows decision tree logic\n",
    "    class Node:\n",
    "        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "            # feature_Index and threshold are determining factors for where the split happens\n",
    "            self.feature_index = feature_index\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    # Recursive binary splitting\n",
    "    # Finds the best feature and threshold to split the data at each noted based on minimizing MSE\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # x - features, y - target, depth - depth of tree\n",
    "        # Continues to split until the maximum specified depth is reached\n",
    "        num_samples, num_features = X.shape\n",
    "        # Stopping condition: if the current depth exceeds the max depth set during initialization or the dataset cannot be split further\n",
    "        if depth >= self.max_depth or num_samples <= 1:\n",
    "            # Create a leaf node with the calculated value\n",
    "            leaf_value = self._calculate_leaf_value(y)\n",
    "            # Usually averages the target values in regression trees\n",
    "            return self.Node(value=leaf_value)\n",
    "\n",
    "        # Finds the best features by iterating through and selecting lowest MSE, calling find best split to find best feature/threshold\n",
    "        best_feature, best_threshold = self._find_best_split(X, y, num_samples, num_features)\n",
    "        # In scenarios where the MSE cannot be further decreased (no more effective split found)\n",
    "        if best_feature is None:\n",
    "            # If no split can improve the outcome, create a leaf node\n",
    "            return self.Node(value=self._calculate_leaf_value(y))\n",
    "        \n",
    "        # Split the dataset and recursively build left and right subtrees (<= left or > right)\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
    "        # Recursively calls build tree to construct subtrees on left and right increasing depth by 1 each time\n",
    "        left_subtree = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return self.Node(feature_index=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def _find_best_split(self, X, y, num_samples, num_features):\n",
    "        # Initializes variables to store best feature index\n",
    "        best_feature, best_threshold = None, None\n",
    "        # Used to insure any inital real MSE found is lower than this starting value\n",
    "        best_mse = np.inf\n",
    "        # Iterates through each feature in dataset X by index\n",
    "        for feature_index in range(num_features):\n",
    "            # extracts unique values of current features as potential thresholds for splitting, will change in future for midpoints or using minimizer\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            # SCIPI minimizer --- TODO\n",
    "            # Iterates through each unique threshold in current thresholds\n",
    "            for threshold in thresholds:\n",
    "                # Calls split function to divide dataset into two subsets based on threshold, left - where features are <= threshold, right - > threshold\n",
    "                left_idxs, right_idxs = self._split(X[:, feature_index], threshold)\n",
    "                # Check for if one subset becomes empty (all features fall onto right or left side)\n",
    "                if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                    continue\n",
    "                mse = self._calculate_mse(y[left_idxs], y[right_idxs])\n",
    "                # If MSE of current split is lower than best MSE found so far... \n",
    "                # Updates best MSE with new value and records the feature/ threshold of this achieved MSE\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    # Splits dataset into left/right based on threshold of given feature\n",
    "    def _split(self, feature_values, threshold):\n",
    "        left_idxs = np.where(feature_values <= threshold)[0]\n",
    "        right_idxs = np.where(feature_values > threshold)[0]\n",
    "        return left_idxs, right_idxs\n",
    "    \n",
    "    def _calculate_leaf_value(self, y):\n",
    "        # The leaf value can be the mean of the target values, calculates that mean value for the leaf node\n",
    "        return np.mean(y)\n",
    "\n",
    "    def _calculate_mse(self, left_y, right_y):\n",
    "        # Calculate the MSE of the left and right splits by weighted averages of variance\n",
    "        total_left_mse = np.var(left_y) * len(left_y) if len(left_y) > 0 else 0\n",
    "        total_right_mse = np.var(right_y) * len(right_y) if len(right_y) > 0 else 0\n",
    "        total_mse = (total_left_mse + total_right_mse) / (len(left_y) + len(right_y))\n",
    "        return total_mse\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predictions array to store predictions for each sample in X\n",
    "        predictions = np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "        return predictions\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        # Recursive method to traverse the tree for a single sample 'x' until a leaf node is reached\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_np = X_train.to_numpy() if isinstance(X_train, pd.DataFrame) else X_train\n",
    "# y_train_np = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train\n",
    "# X_test_np = X_test.to_numpy() if isinstance(X_test, pd.DataFrame) else X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4386820326080767"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tree1 = DecisionTree()\n",
    "# Tree1.fit(X_train_np, y_train_np)\n",
    "# y_pred = Tree1.predict(X_test_np)\n",
    "# my_r2(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _find_best_split(self, X, y, num_samples, num_features):\n",
    "#     best_feature, best_threshold = None, None\n",
    "#     best_mse = np.inf\n",
    "#     # Function to calculate MSE for a given threshold\n",
    "#     def mse_for_threshold(threshold, feature_index):\n",
    "#         left_idxs, right_idxs = self._split(X[:, feature_index], threshold)\n",
    "#         if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "#             return np.inf  # Return a large error if split is not feasible\n",
    "#         return self._calculate_mse(y[left_idxs], y[right_idxs])\n",
    "\n",
    "#     for feature_index in range(num_features):\n",
    "#         # Initial guess for the threshold can be the mean of the feature values\n",
    "#         initial_guess = np.mean(X[:, feature_index])\n",
    "#         # Bounds for the minimizer to ensure it stays within min and max of feature values\n",
    "#         bounds = [(np.min(X[:, feature_index]), np.max(X[:, feature_index]))]\n",
    "#         # The minimization process for the current feature\n",
    "#         result = minimize(mse_for_threshold, x0=initial_guess, args=(feature_index,), bounds=bounds, method='L-BFGS-B')\n",
    "#         if result.fun < best_mse:\n",
    "#             best_mse = result.fun\n",
    "#             best_feature = feature_index\n",
    "#             best_threshold = result.x[0]\n",
    "#     return best_feature, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoost:\n",
    "    def __init__(self, n_estimators: int = 25, max_depth: int = 1, learning_rate: int =.5):\n",
    "        self.max_depth = max_depth # Max depth of the trees\n",
    "        self.n_estimators = n_estimators # Number of trees\n",
    "        self.learning_rate = learning_rate # Learning rate, step size for parameter update\n",
    "        self.trees = [] # List of our trees\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "            # Convert X_train and y_train to NumPy arrays if they aren't already\n",
    "        X_train = np.asarray(X_train)\n",
    "        y_train = np.asarray(y_train)\n",
    "        # To start all residuals = y_train\n",
    "        residuals = np.copy(y_train)\n",
    "        self.f_hat = 0 \n",
    "        # Now time to make decision trees\n",
    "        for i in range(self.n_estimators):\n",
    "            # Build and Fit Tree to data\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X_train, residuals)\n",
    "            # Save our tree\n",
    "            self.trees.append(tree)\n",
    "            # Make prediction\n",
    "            f_hat_b = tree.predict(X_train)\n",
    "            # Update f_hat\n",
    "            self.f_hat += (self.learning_rate*f_hat_b) \n",
    "            # Update residuals\n",
    "            residuals = residuals - (self.learning_rate*f_hat_b)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_hat = np.zeros((X_test.shape[0], ))\n",
    "        for tree in self.trees:\n",
    "            y_hat += self.learning_rate*tree.predict(X_test)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LukeTeitell\\AppData\\Local\\Temp\\ipykernel_4212\\2864869230.py:12: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  return df[mask1][mask3].reset_index()\n"
     ]
    }
   ],
   "source": [
    "df = clean_outliers(df, 'MEDV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['RM',  'TAX', 'PTRATIO', 'LSTAT', 'INDUS']]\n",
    "y = df['MEDV']\n",
    "X_train, X_test, y_train, y_test = my_train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoost(max_depth=2, n_estimators=20)\n",
    "model.fit(X_train=X_train.to_numpy(), y_train=y_train.to_numpy())\n",
    "y_pred = model.predict(X_test.to_numpy())\n",
    "print(my_mse(y_test, y_pred))\n",
    "print(my_mae(y_test, y_pred))\n",
    "print(my_r2(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes in dictionary of parameters to explore\n",
    "# def grid_search(X_train, y_train, X_test, y_test, param_grid):\n",
    "#     # Track lowest MSE seeen so far, set to infinity initially so every value is lower\n",
    "#     # Params stores best paramaters found through best score\n",
    "#     best_score = float('inf')\n",
    "#     best_params = {}\n",
    "\n",
    "#     # Nested loops to iterate over every combination of estimators depth and LR\n",
    "#     for n_estimators in param_grid['n_estimators']:\n",
    "#         for max_depth in param_grid['max_depth']:\n",
    "#             for learning_rate in param_grid['learning_rate']:\n",
    "#                 # Initalize and fit model with every combination of parameters\n",
    "#                 model = GradientBoost(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate)\n",
    "#                 model.fit(X_train, y_train)\n",
    "\n",
    "#                 # Make predictions on the test set and calculate MSE\n",
    "#                 y_pred = model.predict(X_test)\n",
    "#                 score = my_mse(y_test, y_pred)\n",
    "\n",
    "#                 # Update best_score and best_params if current model is better\n",
    "#                 if score < best_score:\n",
    "#                     best_score = score\n",
    "#                     best_params = {'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate}\n",
    "\n",
    "#     return best_params, best_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_val_test_split(*arrays, test_size=0.2, val_size=0.2, random_state=None):\n",
    "    if test_size + val_size >= 1:\n",
    "        raise ValueError('Sum of test_size and val_size must be less than 1.')\n",
    "    for i in arrays:\n",
    "        if len(i) != len(arrays[0]):\n",
    "            raise ValueError('Input arrays must have the same length.')\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    array_size = len(arrays[0])\n",
    "    num_test = math.floor(array_size * test_size)\n",
    "    num_val = math.floor(array_size * val_size)\n",
    "    num_train = array_size - num_test - num_val\n",
    "    \n",
    "    indices = list(range(array_size))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train+num_val]\n",
    "    test_indices = indices[num_train+num_val:]\n",
    "    \n",
    "    return_list = []\n",
    "    for array in arrays:\n",
    "        train_array = array.iloc[train_indices]\n",
    "        val_array = array.iloc[val_indices]\n",
    "        test_array = array.iloc[test_indices]\n",
    "        \n",
    "        return_list.extend([train_array, val_array, test_array])\n",
    "    \n",
    "    return return_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train, X_val, y_val, param_grid):\n",
    "    best_score = float('inf')\n",
    "    best_params = {}\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                model = GradientBoost(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                score = my_mse(y_val, y_pred)\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_depth': max_depth,\n",
    "                        'learning_rate': learning_rate\n",
    "                    }\n",
    "    return best_params, best_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[['RM', 'TAX', 'PTRATIO', 'LSTAT', 'INDUS']]\n",
    "# y = df['MEDV']\n",
    "\n",
    "# # Assuming your DataFrame is `df` and you have defined `X` and `y`\n",
    "# split_data = my_train_val_test_split(X, y, test_size=0.2, val_size=0.2, random_state=42)\n",
    "\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = split_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train_np = np.asarray(X_train)\n",
    "# y_train_np = np.asarray(y_train)\n",
    "# X_test_np = np.asarray(X_test)\n",
    "# y_test_np = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.1}\n",
      "Best MSE Score: 8.76666961978564\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "split_data = my_train_val_test_split(X, y, test_size=0.2, val_size=0.2, random_state=42)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data\n",
    "\n",
    "# Perform the grid search using the training and validation sets\n",
    "best_params, best_score = grid_search(X_train.to_numpy(), y_train.to_numpy(), X_val.to_numpy(), y_val.to_numpy(), param_grid)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best MSE Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m y_test_np \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Perform the grid search using the training and validation sets\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m best_params, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest MSE Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_score)\n",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m, in \u001b[0;36mgrid_search\u001b[1;34m(X_train, y_train, X_val, y_val, param_grid)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m learning_rate \u001b[38;5;129;01min\u001b[39;00m param_grid[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m GradientBoost(n_estimators\u001b[38;5;241m=\u001b[39mn_estimators, max_depth\u001b[38;5;241m=\u001b[39mmax_depth, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m     10\u001b[0m     score \u001b[38;5;241m=\u001b[39m my_mse(y_val, y_pred)\n",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m, in \u001b[0;36mGradientBoost.fit\u001b[1;34m(self, X_train, y_train)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Build and Fit Tree to data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     tree \u001b[38;5;241m=\u001b[39m DecisionTree(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth)\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresiduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Save our tree\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees\u001b[38;5;241m.\u001b[39mappend(tree)\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 46\u001b[0m, in \u001b[0;36mDecisionTree._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     44\u001b[0m left_idxs, right_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(X[:, best_feature], best_threshold)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Recursively calls build tree to construct subtrees on left and right increasing depth by 1 each time\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m left_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_idxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_idxs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m right_subtree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_tree(X[right_idxs, :], y[right_idxs], depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNode(feature_index\u001b[38;5;241m=\u001b[39mbest_feature, threshold\u001b[38;5;241m=\u001b[39mbest_threshold, left\u001b[38;5;241m=\u001b[39mleft_subtree, right\u001b[38;5;241m=\u001b[39mright_subtree)\n",
      "Cell \u001b[1;32mIn[12], line 37\u001b[0m, in \u001b[0;36mDecisionTree._build_tree\u001b[1;34m(self, X, y, depth)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNode(value\u001b[38;5;241m=\u001b[39mleaf_value)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Finds the best features by iterating through and selecting lowest MSE, calling find best split to find best feature/threshold\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m best_feature, best_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_best_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# In scenarios where the MSE cannot be further decreased (no more effective split found)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_feature \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# If no split can improve the outcome, create a leaf node\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 58\u001b[0m, in \u001b[0;36mDecisionTree._find_best_split\u001b[1;34m(self, X, y, num_samples, num_features)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Iterates through each feature in dataset X by index\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_features):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# extracts unique values of current features as potential thresholds for splitting, will change in future for midpoints or using minimizer\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m     thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# SCIPI minimizer --- TODO\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# Iterates through each unique threshold in current thresholds\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m threshold \u001b[38;5;129;01min\u001b[39;00m thresholds:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;66;03m# Calls split function to divide dataset into two subsets based on threshold, left - where features are <= threshold, right - > threshold\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\LukeTeitell\\Venvs\\personal_venv\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[0;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LukeTeitell\\Venvs\\personal_venv\\Lib\\site-packages\\numpy\\lib\\arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[0;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 336\u001b[0m     ar\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[0;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have defined the function my_train_val_test_split above and updated your grid_search function to use validation data\n",
    "X = df[['RM', 'TAX', 'PTRATIO', 'LSTAT', 'INDUS']]\n",
    "y = df['MEDV']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "split_data = my_train_val_test_split(X, y, test_size=0.2, val_size=0.2, random_state=42)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data\n",
    "\n",
    "# Convert pandas DataFrames/Series to numpy arrays if your model requires numpy input\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "X_val_np = X_val.to_numpy()\n",
    "y_val_np = y_val.to_numpy()\n",
    "X_test_np = X_test.to_numpy()\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "# Perform the grid search using the training and validation sets\n",
    "best_params, best_score = grid_search(X_train_np, y_train_np, X_val_np, y_val_np, param_grid)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best MSE Score:\", best_score)\n",
    "\n",
    "# Optional: After finding the best parameters, retrain your model on the combined training and validation set,\n",
    "# and then evaluate it on the test set to assess its final performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
