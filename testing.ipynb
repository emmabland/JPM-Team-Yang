{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and modules we will need for all 3 Tasks are listed here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math \n",
    "# All sklearn functions below are for part 1 and 3, they are not used in part 2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296.0   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242.0   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242.0   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222.0   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "df = pd.read_csv('housing.csv', sep = '\\s+', header = None, names = column_names) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_removal(df, column: str):\n",
    "    \"\"\"Removes rows with outliers in a particular column\n",
    "\n",
    "    Args:\n",
    "        df (pandas DataFrame): The DataFrame we want the values removed from\n",
    "        column (str): The column we're checking for outliers\n",
    "\n",
    "    Returns:\n",
    "        df (pandas DataFrame): The DataFrame minus the outliers\n",
    "    \"\"\"\n",
    "    # Calculate IQR\n",
    "    upper_QR = df[column].quantile(0.75)\n",
    "    lower_QR = df[column].quantile(0.25)\n",
    "    inter_QR = upper_QR-lower_QR\n",
    "    # Filter for outliers\n",
    "    df = df[df[column]<(upper_QR+(1.5*inter_QR))] # Gets all values below upper outlier limits\n",
    "    df = df[df[column]>(lower_QR-(1.5*inter_QR))] # Gets all values above lower limits\n",
    "    # Reset indexes \n",
    "    df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = outlier_removal(df, 'MEDV')\n",
    "\n",
    "X = df[['INDUS', 'RM', 'TAX', 'PTRATIO', 'LSTAT']]\n",
    "y = df['MEDV']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for i in range(10): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n",
    "    model = GradientBoostingRegressor()\n",
    "    # Implement GridSearchCV to optimize hyper-parameters\n",
    "    params = {'min_samples_leaf':[5,10,15,20,25,30], 'max_depth':[2,3,4,5,6,7,8,9,10]} # Params for it to cycle through\n",
    "    clf = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring= 'r2')\n",
    "    # Fit model \n",
    "    clf.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Print model performance metrics\n",
    "    a.append(r2_score(y_test, y_pred))\n",
    "print(np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 5.272439254710651\n",
      "root mean squared error: 2.2961792732081374\n",
      "mean absolute error: 1.7621322476414958\n",
      "r2 score: 0.8646045009428446\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "# Implement GridSearchCV to optimize hyper-parameters\n",
    "params = {'min_samples_leaf':[5,10,15,20,25,30], 'max_depth':[2,3,4,5,6,7,8,9,10]} \n",
    "param_grid = {'n_estimators': [10, 20, 50, 100],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(estimator=model, param_grid=params, cv=5)\n",
    "# Fit model \n",
    "clf.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "my_performance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Class\n",
    "\n",
    "# Used as a week learned within the gradient boosting ensemble (builds iteratively which is why there is a max depth of 1)\n",
    "# High bias but low variance, \n",
    "# Ideal for boosting methods that aim to iteratively reduce error by focusing on hard-to-predict instances.\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    # Each node is a decision point for traversing through the decision tree\n",
    "    # Follows decision tree logic\n",
    "    class Node:\n",
    "        def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "            # feature_Index and threshold are determining factors for where the split happens\n",
    "            self.feature_index = feature_index\n",
    "            self.threshold = threshold\n",
    "            self.left = left\n",
    "            self.right = right\n",
    "            self.value = value\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    # Recursive binary splitting\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        # Continues to split until the maximum specified depth is reached\n",
    "        num_samples, num_features = X.shape\n",
    "        # Stopping condition: if the current depth exceeds the max depth or the dataset cannot be split further\n",
    "        if depth >= self.max_depth or num_samples <= 1:\n",
    "            leaf_value = self._calculate_leaf_value(y)\n",
    "            # Create a leaf node with the calculated value\n",
    "            return self.Node(value=leaf_value)\n",
    "\n",
    "        # Finds the best features by iterating through and selecting lowest MSE\n",
    "        best_feature, best_threshold = self._find_best_split(X, y, num_samples, num_features)\n",
    "        if best_feature is None:\n",
    "            # If no split can improve the outcome, create a leaf node\n",
    "            return self.Node(value=self._calculate_leaf_value(y))\n",
    "        \n",
    "        # Split the dataset and recursively build left and right subtrees\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_threshold)\n",
    "        left_subtree = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "        return self.Node(feature_index=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)\n",
    "\n",
    "    def _calculate_leaf_value(self, y):\n",
    "        # The leaf value can be the mean of the target values, calculates that mean value for the leaf node\n",
    "        return np.mean(y)\n",
    "\n",
    "\n",
    "    # def _find_best_split(self, X, y, num_samples, num_features):\n",
    "    #     # Finds the best feature and threshold to split on based on the lowest MSE\n",
    "    #     best_feature, best_threshold = None, None\n",
    "    #     best_mse = np.inf\n",
    "    #     for feature_index in range(num_features):\n",
    "    #         thresholds = np.unique(X[:, feature_index])\n",
    "    #         # Splits the dataset and calculates the MSE for this split\n",
    "    #         for threshold in thresholds:\n",
    "    #             left_idxs, right_idxs = self._split(X[:, feature_index], threshold)\n",
    "    #             if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "    #                 continue\n",
    "    #             mse = self._calculate_mse(y[left_idxs], y[right_idxs])\n",
    "    #             # Update split if current mse is better\n",
    "    #             if mse < best_mse:\n",
    "    #                 best_mse = mse\n",
    "    #                 best_feature = feature_index\n",
    "    #                 best_threshold = threshold\n",
    "    #     return best_feature, best_threshold\n",
    "\n",
    "\n",
    "    def _find_best_split(self, X, y, num_samples, num_features):\n",
    "        # Finds the best feature and threshold to split on based on the lowest MSE\n",
    "        best_feature, best_threshold = None, None\n",
    "        best_mse = np.inf\n",
    "        for feature_index in range(num_features):\n",
    "            sorted_feature_values = np.sort(X[:, feature_index])\n",
    "            # Creates a possible threshold as the midpoint of each consecutive pair of ordered feature values\n",
    "            thresholds = [(sorted_feature_values[i] + sorted_feature_values[i+1]) / 2 for i in range(num_samples - 1)]\n",
    "            # Splits the dataset and calculates the MSE for this split\n",
    "            for threshold in thresholds:\n",
    "                left_idxs, right_idxs = self._split(X[:, feature_index], threshold)\n",
    "                if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                    continue\n",
    "                mse = self._calculate_mse(y[left_idxs], y[right_idxs])\n",
    "                # Update split if current mse is better\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_feature = feature_index\n",
    "                    best_threshold = threshold\n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    # Splits dataset into left/right based on threshold of given feature\n",
    "    def _split(self, feature_values, threshold):\n",
    "        left_idxs = np.where(feature_values <= threshold)[0]\n",
    "        right_idxs = np.where(feature_values > threshold)[0]\n",
    "        return left_idxs, right_idxs\n",
    "\n",
    "    def _calculate_mse(self, left_y, right_y):\n",
    "        # Calculate the MSE of the left and right splits by weighted averages of variance\n",
    "        total_left_mse = np.var(left_y) * len(left_y) if len(left_y) > 0 else 0\n",
    "        total_right_mse = np.var(right_y) * len(right_y) if len(right_y) > 0 else 0\n",
    "        total_mse = (total_left_mse + total_right_mse) / (len(left_y) + len(right_y))\n",
    "        return total_mse\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predictions array to store predictions for each sample in X\n",
    "        predictions = np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "        return predictions\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        # Recursive method to traverse the tree for a single sample 'x' until a leaf node is reached\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boost Class\n",
    "\n",
    "class GradientBoostAll:\n",
    "    def __init__(self, n_estimators: int = 25, max_depth: int = 1, learning_rate: int =.1):\n",
    "        self.max_depth = max_depth # Max depth of the trees\n",
    "        self.n_estimators = n_estimators # Number of trees\n",
    "        self.learning_rate = learning_rate # Learning rate, step size for parameter update\n",
    "        self.trees = [] # List of our trees\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # To start all residuals = y_train\n",
    "        residuals = np.copy(y_train)\n",
    "        self.f_hat = 0 \n",
    "        # Now time to make decision trees\n",
    "        for i in range(self.n_estimators):\n",
    "            # Build and Fit Tree to data\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X_train, residuals)\n",
    "            # Save our tree\n",
    "            self.trees.append(tree)\n",
    "            # Make prediction\n",
    "            f_hat_b = tree.predict(X_train)\n",
    "            # Update f_hat\n",
    "            self.f_hat += (self.learning_rate*f_hat_b) \n",
    "            # Update residuals\n",
    "            residuals = residuals - (self.learning_rate*f_hat_b)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        y_hat = np.zeros((X_test.shape[0], ))\n",
    "        for tree in self.trees:\n",
    "            y_hat += self.learning_rate*tree.predict(X_test)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_test_split(X,y, test_size = 0.2, random_state = None):\n",
    "    #get random seed to allow reproduceability\n",
    "    random.seed(random_state)\n",
    "    #select test indexes from range of total indexes\n",
    "    test_ixs= random.sample(range(len(y)), math.floor(len(y)*test_size))\n",
    "    #return X_train, X_test, y_train, y_test (train sets are total sets - test sets)\n",
    "    return X.drop(test_ixs), X.iloc[test_ixs], y.drop(test_ixs), y.iloc[test_ixs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mse(a, b):\n",
    "    return np.mean((a-b)**2)\n",
    "\n",
    "def my_rmse(a, b):\n",
    "    return math.sqrt(my_mse(a,b))\n",
    "\n",
    "def my_mae(a,b):\n",
    "    return np.mean(np.abs(a-b))\n",
    "\n",
    "def my_r2(a,b):\n",
    "    y_bar = sum(a)/len(a)\n",
    "    ss_res = np.mean((a-b)**2)\n",
    "    ss_tot = np.mean((a-y_bar)**2)\n",
    "    return 1-(ss_res/ss_tot)\n",
    "\n",
    "def my_performance(a,b):\n",
    "    if not isinstance(a, np.ndarray):\n",
    "        a=a.to_numpy()\n",
    "    if not isinstance(b, np.ndarray):\n",
    "        b=b.to_numpy()\n",
    "    print(f'mean squared error: {my_mse(a,b)}')\n",
    "    print(f'root mean squared error: {my_rmse(a,b)}')\n",
    "    print(f'mean absolute error: {my_mae(a,b)}')\n",
    "    print(f'r2 score: {my_r2(a,b)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 6.049886388821874\n",
      "root mean squared error: 2.4596516803852277\n",
      "mean absolute error: 1.835068885142754\n",
      "r2 score: 0.9020275933906642\n"
     ]
    }
   ],
   "source": [
    "my_performance(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.2}\n",
      "Best MSE Score: 7.0576130977505205\n",
      "BEST r2 Score 0.8250992386850639\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "X_train_np = np.asarray(X_train)\n",
    "y_train_np = np.asarray(y_train)\n",
    "X_test_np = np.asarray(X_test)\n",
    "y_test_np = np.asarray(y_test)\n",
    "\n",
    "# Takes in dictionary of parameters to explore\n",
    "def grid_search(X_train, y_train, X_test, y_test, param_grid):\n",
    "    # Track lowest MSE seeen so far, set to infinity initially so every value is lower\n",
    "    # Params stores best paramaters found through best score\n",
    "    best_score = float('inf')\n",
    "    best_params = {}\n",
    "    best_r2 = None\n",
    "\n",
    "    # Nested loops to iterate over every combination of estimators depth and LR\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                # Initalize and fit model with every combination of parameters\n",
    "                model = GradientBoostAll(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Make predictions on the test set and calculate MSE\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = my_mse(y_test, y_pred)\n",
    "                r2 = my_r2(y_test, y_pred)\n",
    "\n",
    "                # Update best_score and best_params if current model is better\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate}\n",
    "                    best_r2 = r2\n",
    "\n",
    "    return best_params, best_score, best_r2\n",
    "    \n",
    "best_params, best_score, best_r2 = grid_search(X_train_np, y_train_np, X_test_np, y_test_np, param_grid)\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best MSE Score:\", best_score)\n",
    "print(\"BEST r2 Score\", best_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 8.529225773441762\n",
      "root mean squared error: 2.920483825232005\n",
      "mean absolute error: 2.251904267837174\n",
      "r2 score: 0.7886299432201218\n"
     ]
    }
   ],
   "source": [
    "X_train_np = np.asarray(X_train)\n",
    "y_train_np = np.asarray(y_train)\n",
    "X_test_np = np.asarray(X_test)\n",
    "y_test_np = np.asarray(y_test)\n",
    "our_model = GradientBoostAll(n_estimators=50, max_depth=2, learning_rate=0.2)\n",
    "our_model.fit(X_train_np, y_train_np)\n",
    "y_pred = our_model.predict(X_test_np)\n",
    "my_performance(y_test_np, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = []\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=i)\n",
    "    X_train_np = np.asarray(X_train)\n",
    "    y_train_np = np.asarray(y_train)\n",
    "    X_test_np = np.asarray(X_test)\n",
    "    y_test_np = np.asarray(y_test)\n",
    "    our_model = GradientBoostAll(n_estimators=100, max_depth=3, learning_rate=0.2)\n",
    "    our_model.fit(X_train_np, y_train_np)\n",
    "    y_pred = our_model.predict(X_test_np)\n",
    "    b.append(my_r2(y_test_np, y_pred))\n",
    "np.mean(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np[1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.561, 4.138, 4.368, 4.519, 4.628, 4.903, 4.906, 4.926, 4.963,\n",
       "       5.   , 5.012, 5.019, 5.036, 5.093, 5.155, 5.272, 5.304, 5.349,\n",
       "       5.362, 5.39 , 5.403, 5.412, 5.414, 5.453, 5.454, 5.456, 5.468,\n",
       "       5.531, 5.536, 5.56 , 5.569, 5.57 , 5.572, 5.594, 5.597, 5.599,\n",
       "       5.604, 5.605, 5.608, 5.613, 5.617, 5.627, 5.628, 5.631, 5.637,\n",
       "       5.682, 5.683, 5.693, 5.705, 5.706, 5.707, 5.708, 5.709, 5.713,\n",
       "       5.727, 5.731, 5.741, 5.747, 5.757, 5.786, 5.79 , 5.794, 5.803,\n",
       "       5.807, 5.813, 5.822, 5.836, 5.837, 5.85 , 5.851, 5.852, 5.854,\n",
       "       5.856, 5.859, 5.868, 5.869, 5.87 , 5.871, 5.872, 5.874, 5.875,\n",
       "       5.876, 5.877, 5.878, 5.879, 5.88 , 5.885, 5.887, 5.888, 5.889,\n",
       "       5.891, 5.895, 5.896, 5.898, 5.905, 5.913, 5.914, 5.92 , 5.926,\n",
       "       5.928, 5.933, 5.935, 5.936, 5.942, 5.95 , 5.951, 5.952, 5.957,\n",
       "       5.96 , 5.961, 5.963, 5.965, 5.966, 5.972, 5.981, 5.983, 5.987,\n",
       "       5.99 , 5.998, 6.003, 6.004, 6.006, 6.009, 6.012, 6.014, 6.015,\n",
       "       6.019, 6.02 , 6.021, 6.023, 6.027, 6.03 , 6.031, 6.037, 6.041,\n",
       "       6.047, 6.064, 6.066, 6.069, 6.072, 6.081, 6.083, 6.086, 6.092,\n",
       "       6.095, 6.096, 6.103, 6.108, 6.112, 6.115, 6.12 , 6.121, 6.122,\n",
       "       6.127, 6.129, 6.13 , 6.137, 6.14 , 6.142, 6.144, 6.145, 6.151,\n",
       "       6.152, 6.162, 6.163, 6.164, 6.167, 6.172, 6.174, 6.176, 6.182,\n",
       "       6.185, 6.193, 6.195, 6.202, 6.208, 6.209, 6.211, 6.212, 6.219,\n",
       "       6.226, 6.229, 6.232, 6.24 , 6.242, 6.245, 6.249, 6.25 , 6.251,\n",
       "       6.266, 6.273, 6.279, 6.29 , 6.301, 6.31 , 6.312, 6.315, 6.316,\n",
       "       6.317, 6.319, 6.326, 6.333, 6.335, 6.341, 6.343, 6.345, 6.358,\n",
       "       6.375, 6.376, 6.377, 6.38 , 6.382, 6.383, 6.389, 6.393, 6.395,\n",
       "       6.398, 6.402, 6.404, 6.405, 6.406, 6.411, 6.416, 6.417, 6.421,\n",
       "       6.425, 6.426, 6.431, 6.433, 6.436, 6.437, 6.438, 6.442, 6.453,\n",
       "       6.454, 6.456, 6.458, 6.461, 6.471, 6.474, 6.481, 6.482, 6.485,\n",
       "       6.49 , 6.495, 6.511, 6.516, 6.525, 6.538, 6.54 , 6.545, 6.546,\n",
       "       6.549, 6.552, 6.556, 6.565, 6.567, 6.575, 6.579, 6.59 , 6.593,\n",
       "       6.595, 6.604, 6.606, 6.616, 6.619, 6.625, 6.629, 6.63 , 6.631,\n",
       "       6.635, 6.642, 6.649, 6.655, 6.674, 6.678, 6.715, 6.718, 6.726,\n",
       "       6.727, 6.728, 6.739, 6.749, 6.762, 6.782, 6.794, 6.8  , 6.812,\n",
       "       6.816, 6.824, 6.826, 6.833, 6.842, 6.849, 6.852, 6.854, 6.86 ,\n",
       "       6.861, 6.871, 6.874, 6.879, 6.897, 6.939, 6.943, 6.951, 6.957,\n",
       "       6.968, 6.975, 6.976, 6.98 , 6.982, 6.998, 7.007, 7.014, 7.024,\n",
       "       7.041, 7.061, 7.079, 7.088, 7.104, 7.135, 7.147, 7.163, 7.178,\n",
       "       7.185, 7.203, 7.206, 7.236, 7.241, 7.274, 7.287, 7.313, 7.327,\n",
       "       7.333, 7.393, 7.412, 7.416, 7.42 , 7.52 , 7.686, 7.765, 7.82 ,\n",
       "       7.853, 8.04 , 8.069, 8.247, 8.259, 8.266, 8.337, 8.398, 8.78 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_train_np[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold Cross Validation function to perform hyperparameter tuning \n",
    "def kfold_cv(X_train, y_train, K_folds = 5, random_state = None, n_estimators: int = 25, max_depth: int = 1, learning_rate: int =.1):\n",
    "    # Create empty lists for r2 and MSE scores\n",
    "    r2_list = []\n",
    "    mse_list = []\n",
    "    # Set random seed for reproduceability\n",
    "    np.random.seed(random_state)\n",
    "    # Create list of indices as length of data\n",
    "    indices = np.arange(len(X_train))\n",
    "    # Shuffle indices\n",
    "    np.random.shuffle(indices)\n",
    "    # Create fold size as length/K\n",
    "    fold_size = math.floor(len(X_train)/K_folds)\n",
    "    for i in range(K_folds-1):\n",
    "        # Create start- and end-points for this fold (final fold will be larger if K does not evenly divide length)\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size if i < K_folds - 1 else len(X_train)\n",
    "        # Assign test indices as those within start- and end-points, train indices as those outside this range\n",
    "        test_indices = indices[start:end]\n",
    "        train_indices = np.concatenate((indices[:start], indices[end:]))\n",
    "        # Create new train and test sets from existing training set based on train and test indices\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        X_test_fold = X_train.iloc[test_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        y_test_fold = y_train.iloc[test_indices]\n",
    "        # Initialize model with hyperparameters to match function input parameters\n",
    "        model = GradientBoostAll(n_estimators= n_estimators, max_depth=max_depth, learning_rate=learning_rate)\n",
    "        # Fit model on fold training data\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        # Model predicts fold test data\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "        # r2 and MSE scores appended to list\n",
    "        r2_list.append(my_r2(y_test_fold, y_pred_fold))\n",
    "        mse_list.append(my_mse(y_test_fold, y_pred_fold))\n",
    "    # Returns average r2 and average MSE across all folds\n",
    "    return np.mean(r2_list), np.mean(mse_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.600530729004401, 17.22816186912361)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_cv(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 5.24605434966139\n",
      "root mean squared error: 2.290426674150777\n",
      "mean absolute error: 1.7862341568945976\n",
      "r2 score: 0.865282061596299\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform grid search using K-fold cross validation to optimize hyperparameters\n",
    "def grid_search_kf(X_train, y_train, X_test, y_test, param_grid, random_seed=None, K_folds = 5):\n",
    "    # Create best score and best parameter variables - best score starts as infinity so any real-valued score will be better (lower)\n",
    "    best_score = float('inf')\n",
    "    best_params = {}\n",
    "\n",
    "    # For every combination of parameters in the parameter grid:\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for learning_rate in param_grid['learning_rate']:\n",
    "                # get r2 and MSE from K-fold function tuned to this combination of hyperparameters\n",
    "                r2, score = kfold_cv(X_train, y_train, n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, random_state=random_seed, K_folds=K_folds)\n",
    "                # If MSE is better than best_score, replace best_score with this score and existing best_params with these\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate}\n",
    "    # Initialize model with best parameters\n",
    "    model = GradientBoostAll(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], learning_rate=best_params['learning_rate'])\n",
    "    # Fit model on training data\n",
    "    model.fit(X_train, y_train)\n",
    "    # Model predicts test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Returns performance metrics from prediction \n",
    "    return my_performance(y_test, y_pred)\n",
    "\n",
    "# Execute grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Assuming X_train, y_train, X_test, and y_test are already defined\n",
    "grid_search_kf(X_train, y_train, X_test, y_test, param_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr, xe, yr, ye = train_test_split(X, y, test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.69327956989247"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.69327956989247"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(yr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
